{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122885b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "train = pd.read_csv(\"aluminum_coldRoll_train.csv\")\n",
    "test  = pd.read_csv(\"aluminum_coldRoll_testNoY.csv\")\n",
    "\n",
    "target_col = \"y_passXtremeDurability\"\n",
    "id_col = \"ID\"\n",
    "\n",
    "X = train.drop(columns=[target_col])\n",
    "y = train[target_col]\n",
    "X_test = test.copy()\n",
    "\n",
    "# ============================================================\n",
    "# 2. ONE-HOT ENCODE CATEGORICALS (FOR BOTH MODELS)\n",
    "# ============================================================\n",
    "\n",
    "cat_cols = [\n",
    "    \"alloy\",\n",
    "    \"cutTemp\",\n",
    "    \"rollTemp\",\n",
    "    \"topEdgeMicroChipping\",\n",
    "    \"blockSource\",\n",
    "    \"machineRestart\",\n",
    "    \"contourDefNdx\",\n",
    "]\n",
    "\n",
    "# get_dummies handles cats; numeric cols pass through\n",
    "X_enc = pd.get_dummies(X, columns=cat_cols)\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_cols)\n",
    "\n",
    "# align columns between train and test\n",
    "X_test_enc = X_test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
    "\n",
    "# ============================================================\n",
    "# 3. TRAIN/VAL SPLIT\n",
    "# ============================================================\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_enc, y, test_size=0.3, random_state=101, stratify=y\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODEL FACTORIES WITH YOUR TUNED-ish SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "def make_xgb(seed):\n",
    "    return XGBClassifier(\n",
    "        n_estimators=450,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1,\n",
    "        min_child_weight=3,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "def make_lgbm(seed):\n",
    "    return LGBMClassifier(\n",
    "        n_estimators=450,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1,\n",
    "        objective=\"binary\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "seeds = [101, 202, 303, 404, 505]\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAIN ENSEMBLES PER MODEL TYPE (XGB & LGBM)\n",
    "# ============================================================\n",
    "\n",
    "val_preds_xgb = []\n",
    "val_preds_lgb = []\n",
    "\n",
    "print(\"Fitting XGBoost and LightGBM models on train split...\")\n",
    "\n",
    "for seed in seeds:\n",
    "    # XGBoost\n",
    "    xgb = make_xgb(seed)\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    val_prob_xgb = xgb.predict_proba(X_val)[:, 1]\n",
    "    val_preds_xgb.append(val_prob_xgb)\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm = make_lgbm(seed)\n",
    "    lgbm.fit(X_tr, y_tr)\n",
    "    val_prob_lgb = lgbm.predict_proba(X_val)[:, 1]\n",
    "    val_preds_lgb.append(val_prob_lgb)\n",
    "\n",
    "val_preds_xgb = np.vstack(val_preds_xgb)  # shape: (n_seeds, n_val)\n",
    "val_preds_lgb = np.vstack(val_preds_lgb)\n",
    "\n",
    "# average within each model type\n",
    "val_mean_xgb = val_preds_xgb.mean(axis=0)\n",
    "val_mean_lgb = val_preds_lgb.mean(axis=0)\n",
    "\n",
    "# ============================================================\n",
    "# 6. SEARCH BEST BLEND WEIGHT ON VALIDATION LOG-LOSS\n",
    "#    blend = w * XGB + (1 - w) * LGBM\n",
    "# ============================================================\n",
    "\n",
    "best_w = None\n",
    "best_ll = float(\"inf\")\n",
    "\n",
    "for w in np.linspace(0.0, 1.0, 51):  # 0.00, 0.02, ..., 1.00\n",
    "    blended = w * val_mean_xgb + (1.0 - w) * val_mean_lgb\n",
    "    ll = log_loss(y_val, blended)\n",
    "    if ll < best_ll:\n",
    "        best_ll = ll\n",
    "        best_w = w\n",
    "\n",
    "print(f\"\\nBest blend weight w for XGB: {best_w:.3f}\")\n",
    "print(f\"Validation log-loss with blend: {best_ll:.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. TRAIN ON FULL DATA AND APPLY SAME BLENDING TO TEST\n",
    "# ============================================================\n",
    "\n",
    "test_preds_xgb = []\n",
    "test_preds_lgb = []\n",
    "\n",
    "print(\"\\nFitting XGBoost and LightGBM models on FULL data...\")\n",
    "\n",
    "for seed in seeds:\n",
    "    # XGBoost full-data model\n",
    "    xgb = make_xgb(seed)\n",
    "    xgb.fit(X_enc, y)\n",
    "    test_prob_xgb = xgb.predict_proba(X_test_enc)[:, 1]\n",
    "    test_preds_xgb.append(test_prob_xgb)\n",
    "\n",
    "    # LightGBM full-data model\n",
    "    lgbm = make_lgbm(seed)\n",
    "    lgbm.fit(X_enc, y)\n",
    "    test_prob_lgb = lgbm.predict_proba(X_test_enc)[:, 1]\n",
    "    test_preds_lgb.append(test_prob_lgb)\n",
    "\n",
    "test_preds_xgb = np.vstack(test_preds_xgb)\n",
    "test_preds_lgb = np.vstack(test_preds_lgb)\n",
    "\n",
    "test_mean_xgb = test_preds_xgb.mean(axis=0)\n",
    "test_mean_lgb = test_preds_lgb.mean(axis=0)\n",
    "\n",
    "# apply best blend weight\n",
    "test_blend = best_w * test_mean_xgb + (1.0 - best_w) * test_mean_lgb\n",
    "\n",
    "# clip for safety\n",
    "eps = 1e-6\n",
    "test_blend = np.clip(test_blend, eps, 1 - eps)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": X_test[id_col],\n",
    "    \"y_passXtremeDurability\": test_blend\n",
    "})\n",
    "\n",
    "submission.to_csv(\"team16_xgb_lgbm_blend.csv\", index=False)\n",
    "print(\"\\nSaved submission as team16_xgb_lgbm_blend.csv\")\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
